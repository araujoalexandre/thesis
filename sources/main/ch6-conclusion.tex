%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Conclusion}
\label{chapter:ch6-conclusion}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\localtoc

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Summary of the Contributions}
\label{section:ch6-summary_of_the_contributions}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


State-of-the-art in a variety of domains, deep neural networks exhibit important limitations.
Indeed, current neural networks tend to be very large in terms of their number of parameters which make them difficult to train and to deploy in real-world applications.
Furthermore, they exhibit instability to small perturbations of their inputs which lead to adversarial attacks. 

In this thesis, we have used structured matrices from the Toeplitz family to make contributions to the field of deep learning.
Our contributions are twofold.
First, we studied deep diagonal-circulant neural networks, which are deep neural networks in which weight matrices are the product of diagonal and circulant ones.
Using diagonal and circulant matrices instead of dense ones allows for an important reduction in the number of parameters which make them more efficient and cost-effective.
In addition to being more compact than fully connected neural networks, diagonal-circulant neural networks have a high expressivity that makes them useful for numerous use cases.
In order to characterize the expressive power of diagonal-circulant neural networks, we build upon the work of~\citet{huhtanen2015factoring} which states that any matrix can be decomposed into a product of alternating diagonal and circulant matrices.
Based on this result, we have successfully demonstrated that neural networks with diagonal and circulant matrices are \emph{universal approximators} and characterized their expressive power with respect to their depth.
We also demonstrated the effectiveness of this class of compact neural networks to video classification with a real-world dataset.

Secondly, we proposed a new regularization scheme of convolutional neural networks that improve their robustness against adversarial attacks.
This regularization scheme reduces a bound of the Lipschitz constant of the neural networks thus making it less sensitive to small perturbations of its input.
This contribution is based on the observation that the convolution operation of convolutional layers can be interpreted as a matrix-multiplication where the matrix is the concatenation of multiple doubly-block Toeplitz matrices.
From the properties of doubly-block Toeplitz matrices and a Fourier representation introduced by~\citet{grenander1958toeplitz}, we demonstrated an upper-bound on the singular values of convolutional layers.
In order to use this upper-bound in a large-scale setting, we introduce the PolyGrid algorithm (see \Cref{algorithm:ch5-polygrid}) which efficiently and accurately computes an approximation of this upper-bound.
Finally, we demonstrated that employing this bound as a regularizer improved the generalization and the robustness of neural networks trained with the adversarial training scheme.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Perspectives and Future Works}
\label{section:ch6-perspectives_and_future_works}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In order to improve upon our work, several ideas follow naturally.
First, the diagonal-circulant decomposition can be used in a wide variety of settings.
In~\Cref{chapter:ch4-diagonal_circulant_neural_network} and Appendix~\ref{appendix:ap2-diagonal_circulant_neural_networks_for_video_classification}, we have used it for compressing embedding layers in the context of video classification.
This decomposition could also be used to compress attention layers of Transformers networks~\cite{vaswani2017attention} where the attention layer is described as follows:
% The attention layer is defined as follows:
\begin{equation}
  \text{Attention}(\Qmat, \Kmat, \Vmat) = \text{softmax} \left( \frac{\Qmat \Kmat^\top}{\sqrt{d_k}} \right) \Vmat \enspace,
\end{equation}
where $\Qmat, \Kmat$ and $\Vmat$ are dense matrices.
Taking this layer as a building block leads to large neural networks as demonstrated by the GPT-3 architecture with 96 attention layers and 175 billion parameters.
Secondly, the regularization of the Lipschitz constant of neural networks has been a growing interest in the training of neural networks.
Indeed, Lipschitz regularization goes beyond robustness of neural networks and can be used in a wide variety of settings, for example, to stabilize the training of Generative Adversarial Networks and invertible networks, or to improve generalization capabilities of classifiers.
In \Cref{chapter:ch5-lipschitz_bound}, we have proposed a layer-wise approach and focused on the Lipschitz constant of convolutional layers.
As future work, we could consider neural networks as nonlinear Lipschitz continuous operators.
For example, for a two-layers neural networks $N(\xvec) = \Wmat^{(2)} \rho(\Wmat^{(1)} \xvec)$,
we could define the operator:
\begin{equation}
  \Mmat(\xvec) = \Wmat^{(2)} \Amat (\Wmat^{(1)} \xvec) \Wmat^{(1)}
\end{equation}
where $\Amat(\xvec) = \diag(\rho'(\xvec))$ is a diagonal matrix dependent of $\xvec$.
Then, it is easy to see that $N(\xvec) = \Mmat(\xvec) \xvec$.
The spectrum of this kind of operator has been studied by Kachurovskij in 1969 (see \citet{appell2008nonlinear}). 
More precisely, the set 
\begin{equation}
  \mu(\Mmat) \triangleq \left\{ \lambda \in \Cbb \mid \lambda \Imat - \Mmat \text{ is a lipeomorphism} \right\} 
\end{equation} 
is called the \emph{Kachurovskij resolvent} set of $\Mmat$ and its complement
\begin{equation}
  \sigma(\Mmat) \triangleq \Cbb \setminus \mu(\Mmat) \enspace, 
\end{equation}
the \emph{Kachurovskij spectrum} of $\Mmat$ and where a nonlinear Lipschitz continuous operator is a \emph{lipeomorphism} if its inverse is also nonlinear Lipschitz continuous.
A study of this spectrum could provide some information on the behavior of neural networks.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Discussion}
\label{section:ch6-discussion}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Although our contributions offer concrete techniques for building compact and reliable neural networks, they also highlight some important difficulties in training neural networks. 
First, if we discard techniques such as pruning or quantization for building compact neural networks due to the necessity of training a large neural network prior to compression, designing parameters-efficient neural networks that are compact \emph{by nature} require rethinking the whole architecture.
For computer vision tasks, the convolution operation is a compact and powerful transform, however, we still haven't found such equivalent transform for other use cases.
For example, the attention layer is a powerful transform for NLP tasks but not very efficient in terms of parameters and complexity.
Secondly, while new techniques successfully improve the robustness of neural networks, it has been shown that accuracy and robustness are actually at odds \cite{zhang2019theoretically}, meaning that reaching a perfect robustness while maintaining the current accuracy might be impossible in the current setting.



