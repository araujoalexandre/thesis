%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Background}
\label{chapter:ch2-background}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\localtoc


\vspace{\fill}

This chapter gives a short introduction on the theory of Toeplitz matrices~\cite{gray2006toeplitz} and on supervised learning and neural networks~\cite{shalev2014understanding}.
First, we describe the mathematical properties of Toeplitz matrices and known theorems that we use in this thesis. 
In linear algebra, a Toeplitz matrix, named after Otto Toeplitz, is a matrix in which each descending diagonal, from left to right, is constant.
We will use a number of these results in the context of neural networks.
Neural networks are parametric functions trained to map an input to an output.
For example, an image (input) to the content of the image (output).
The second section of this chapter is divided into three parts.
First, we review notions of supervised learning which refers to the problem of optimizing the parameters of a function in order to map an input to an output based on a series of input-output pairs.
Then, we formally define neural networks and some of their properties.
We finish by presenting the concept of adversarial examples which we will use in \Cref{chapter:ch5-lipschitz_bound}.

\vspace{\fill}

% In the statistical learning framework, supervised learning refers to the problem of optimizing the parameters of a function in order to map an input to an output based on a series of input-output pairs.
% For example, an image (input) mapped to a class (output) describing its content.
% The second section describes a formal model that aims to describe such learning tasks.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{A Primer on Toeplitz and Circulant Matrices}
\label{section:ch2-a_primer_on_toeplitz_and_circulant_matrices}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{sources/main/ch2-background_toeplitz}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Supervised Learning \& Neural Networks}
\label{section:ch2-supervised_learning_neural_networks}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\input{sources/main/ch2-background_neural_networks}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Summary of the Chapter}
\label{section:ch2-summary_of_the_background}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

As explained in the Introduction (\Cref{chapter:ch1-introduction}), our contributions lie at the intersection between neural networks and structured matrices.
In this chapter, we have reviewed the necessary concepts to present our contributions and some related work.

First, \Cref{section:ch2-a_primer_on_toeplitz_and_circulant_matrices} introduce Toeplitz and Circulant matrices which are the main mathematical objects used in this thesis.
Toeplitz and circulant matrices are structured matrices in which each descending diagonal, from left to right, is constant.
These structured matrices are the building blocks of our contribution on compact neural networks (\Cref{chapter:ch4-diagonal_circulant_neural_network}) and enables fast approximation of the Lipschitz constant of convolutional layers leading to a new regularization scheme (\Cref{chapter:ch5-lipschitz_bound}).

Finally, in \Cref{subsection:ch2-introduction_on_supervised_learning}, we gave a quick overview of the concept of supervised learning, which presents the mathematical tools for optimizing a parameterized function in order to map an input to an output based on a series of input-output pairs. 
Although the statistical learning framework considers generic hypothesis space, in this work we use a class of function called neural networks presented in \Cref{subsection:ch2-preliminaries_on_neural_networks}.
We also present, in \Cref{subsection:ch2-preliminaries_on_adversarial_attacks}, the concept of adversarial attacks and robustness of neural networks.
We show how a neural network can be sensitive to small perturbations to its input and thus vulnerable to adversarial examples.
Reducing the sensitivity and therefore increasing the robustness of neural networks is the central theme of our second contribution presented in \Cref{chapter:ch5-lipschitz_bound}.





