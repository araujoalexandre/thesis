
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{The Global Lipschitz Constant of Neural Networks}
\label{subsection:ch3-the_global_lipschitz_constant_of_neural_networks}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\noindent
The regularization of the Lipschitz constant of neural networks has seen a growing interest in the last few years.
Indeed, numerous results have shown that neural networks with a low Lipschitz constant exhibit better generalization~\cite{bartlett2017spectrally} and higher robustness to adversarial attacks~\cite{szegedy2013intriguing,tsuzuku2018lipschitz, farnia2018generalizable}.

The Lipschitz constant, defined in~\Cref{definition:ch2-lipschitz_constant}, is a measure of the stability of the network.
If the Lipschitz constant is high, the network will tend to be more sensitive to input perturbations, meaning, if the input changes by $\epsilon$, the output changes by at most $k\epsilon$.
The Lipschitz constant of a function can also be expressed using the differential operator as follows:
\begin{theorem}[Rademacher's Theorem] \label{theorem:ch3-lipschitz_differential_op}
  If $f: \Rbb^n \rightarrow \Rbb^m$ is a Lipschitz continuous function, then $f$ is differentiable almost everywhere.
  Moreover, if $f$ is Lipschitz continuous, then
  \begin{align}
    \lip{f} = \sup_{\xvec \in \Rbb^n} \norm{\mathrm{D}_\xvec f(\xvec)}_2
  \end{align}
  where $\mathrm{D}_\xvec$ is the differential operator of $f$ at $\xvec$.
\end{theorem}

\citet{tsuzuku2018lipschitz} have studied the relationship between the robustness and the Lipschitz constant and the margin of neural networks. 
By the definition of the Lipschitz constant, we have the following:
\begin{equation}
  \norm{N_\Omega(\xvec) - N_\Omega(\xvec + \adv)}_2 \leq \lip{N_\Omega} \norm{\adv}_2
\end{equation}
Recall the margin operator $\margin: \Rbb^k \times [k] \rightarrow \Rbb$ from~\Cref{subsection:ch2-recent_results_on_the_theory_of_neural_networks} defined as:
\begin{equation}
  \margin(\vvec, j) \triangleq \vvec_j - \max_{i \neq j} \vvec_i
\end{equation}
Then, we have the following proposition which characterizes the robustness of a neural network with respect to its margin and Lipschitz constant.
\begin{proposition}[\citet{tsuzuku2018lipschitz}]
  \begin{equation} \label{equation:ch3-margin_guarded_area}
    \margin \big( N_\omega(\xvec), y \big) \geq \sqrt{2} \lip{N} \norm{\adv}_2 \quad \Longrightarrow \quad \margin \big( N_\Omega(\xvec + \adv), y \big) \geq 0
  \end{equation}
  \removespace
\end{proposition}
\noindent
If the inequality on the right-hand side of \Cref{equation:ch3-margin_guarded_area} is verified then the adversarial margin is positive, \ie, the network correctly predicts the label. 
From this proposition, we can conclude that for a given neural network with specific margins, a lower Lipschitz constant allows for an increase in robustness. 
Note that the margin is already maximized in a multi-class setting with the cross-entropy loss as stated in~\citet{hein2017formal}.
A multitude of work have tried to reduce the Lipschitz constant in order to improve adversarial robustness.
However, \citet{scaman2018lipschitz} have shown that computing the exact Lipschitz constant of a neural network is NP-hard.
The following theorem shows that, even for shallow neural networks, exact Lipschitz computation is not achievable in polynomial time:
\begin{theorem}[\citet{scaman2018lipschitz}] \label{theorem:ch3-lipschitz_computation}
  Let us define the problem associated with the exact computation of the Lipschitz constant of a $2$-layer neural network with $\relu$ activation:
  \begin{itemize}%[topsep=0pt,noitemsep]
    \item[] \textbf{Input:} Two matrices $\Wmat^{(1)} \in \Rbb^{l \times n}$ and $\Wmat^{(2)} \in \Rbb^{m \times l}$, and a constant $c \geq 0$.
    \item[] \textbf{Question:} Let $N = \Wmat^{(2)} \circ \rho \circ \Wmat^{(1)}$ where $\rho$ is the $\relu$ activation function. \emph{Is the Lipschitz constant $\lip{N} \leq c$ ?}
  \end{itemize}
  Then, assuming that $\textbf{P} \neq \textbf{NP}$, the problem above is NP-hard. 
\end{theorem}


\noindent
To overcome this difficulty, researchers have relied on devising a tight upper bound of the Lipschitz constant.
For example, \citet{scaman2018lipschitz} have shown that the Lipschitz constant of a neural network $N$ can be explicitly formulated using \Cref{theorem:ch3-lipschitz_differential_op} and the chain rule:
\begin{equation} \label{equation:ch3-decomposition_jacobian_lipschitz}
  \lip{\nn} = \sup_{x \in \Rbb^n} \norm{\Wmat^{(p)} \diag(\rho'_\depth(\theta_\depth)) \dots \Wmat^{(2)} \diag(\rho'_1(\theta_1)) \Wmat^{(1)}}_2,
\end{equation}
where $\theta_i = \layer^{\act_i}_{\Wmat^{(i)}, \bvec^{(i)}} \circ \cdots \circ \layer^{\act_1}_{\Wmat^{(1)}, \bvec^{(1)}}(\xvec)$ is the intermediate output after $i$ layers.
The Lipschitz of the neural network $N$ can then be upper bounded as follows:
\begin{align}
  \lip{\nn} &\leq \max_{\forall i,\ \sigma_i \in [0, 1]^{w^{(i+1)}}} \norm{\Wmat^{(\depth)} \diag(\sigma_{\depth-1}) \dots \diag(\sigma_1) \Wmat^{(1)}}_2 \notag \\
  &\leq \max_{\forall i,\ \sigma_i \in [0, 1]^{w^{(i+1)}}} \norm{ \pmb{\Sigma}^{(\depth)} \Vmat^{(\depth)\top} \diag(\sigma_{\depth-1}) \dots \diag(\sigma_1) \Umat^{(1)} \pmb{\Sigma}^{(1)}}_2 \notag \\
  &\leq \prod_{i=1}^{\depth-1} \max_{\sigma_i \in [0, 1]^{w^{(i+1)}}} \norm{\widetilde{\pmb{\Sigma}}^{(i+1)} \Vmat^{(i+1)\top} \diag(\sigma_{i+1}) \Umat^{(i)} \widetilde{\pmb{\Sigma}}^{(i)}}_2 
\end{align}
where $\widetilde{\pmb{\Sigma}}^{(i)} = \pmb{\Sigma}^{(i)}$ if $i \in \{1, \depth\}$ and $\widetilde{\pmb{\Sigma}}^{(i)} = {\pmb{\Sigma}^{(i)}}^{1/2}$ otherwise.
The first inequality is due to the fact that the derivatives of the activation functions are bounded, \ie, $\rho_i(\xvec) \in [0, 1]^{w^{(i+1)}}$, the second inequality is obtained by decomposing each weight matrix $\Wmat^{(i)}$ with the \emph{Singular Value Decomposition} such that $\Wmat^{(i)} = \Umat^{(i)} \pmb{\Sigma}^{(i)} \Vmat^{(i)\top}$; and finally, the last inequality is due to the submultiplicativity of the operator norm.
Although accurate, this bound is still computationally expensive to compute due to the singular value decomposition and the optimization for each layer. 
In the same line of research, recent work~\cite{fazlyab2019safety,fazlyab2019efficient,latorre2020lipschitz} has proposed a tight bound on the Lipschitz constant of the full network with the use of semi-definite programming.
More precisely, \citet{fazlyab2019efficient} have demonstrated the following result:
\begin{theorem}[Lipschitz bounds \citet{fazlyab2019efficient}] \label{theorem:ch3-lipschite_semidefinite_programming}
  Consider a neural network $N: \Rbb^n \rightarrow \Rbb^m$ such that $N(\xvec) = \Wmat^{(2)} \rho(\Wmat^{(1)} \xvec + \bvec^{(1)}) + \bvec^{(2)}$.
  Suppose the activation function $\rho$ is \emph{slope-restricted} in the sector $[\alpha,\beta]$, \ie,
  \begin{equation}
    \alpha \leq \frac{\rho(y) - \rho(x)}{y-x} \leq \beta \quad \forall x,y \in \Rbb. 
  \end{equation}
  Define the set $\mathcal{T}_{n}$ as the following:
  \begin{equation*}
    \mathcal{T}_n = \{\Tmat \in \Sbb^n \mid \Tmat = \sum_{i=1}^{n} \lambda_{ii} \evec^{(i)} \evec^{(i)\top} + \sum_{1 \leq i<j \leq n} \lambda_{ij}(\evec^{(i)} - \evec^{(j)})(\evec^{(i)}-\evec^{(j)})^\top, \lambda_{ij} \geq 0 \}.
  \end{equation*}
  where $\Sbb^d$ is the set of all symmetric matrices of size $n \times n$.
  Suppose there exists a constant $c>0$ such that the matrix inequality
  \begin{align}
    \Mmat(c,\Tmat) \triangleq
      \leftmatrix
      -2\alpha \beta \Wmat^{(1)\top} \Tmat \Wmat^{(1)} - c \Imat_n & (\alpha+\beta) \Wmat^{(1)\top} \Tmat  \\
      (\alpha+\beta) \Tmat \Wmat^{(1)} & -2\Tmat+\Wmat^{(2)\top} \Wmat^{(2)}
      \rightmatrix
      \leq 0,
  \end{align}
  holds for some $\Tmat \in \mathcal{T}_{n}$. Then $\norm{N(\xvec)-N(\yvec)}_2 \leq \sqrt{c} \norm{\xvec-\yvec}_2$ for all  $\xvec,\yvec \in \Rbb^n$.
\end{theorem}
\noindent
From \Cref{theorem:ch3-lipschite_semidefinite_programming}, the constant $c$ is an upper bound on the Lipschitz constant of the network.
The authors proposed to find the tightest bound by solving the following optimization problem (Semidefinite Program):
\begin{align}
  \textrm{minimize} \quad c \quad \text{ subject to} \quad \Mmat(c,\Tmat) \leq 0 \quad \text{and} \quad \Tmat \in \mathcal{T}_{n},
\end{align}
where the decision variables are $(c,\Tmat) \in \Rbb_+ \times \mathcal{T}_n$.
Note that $\Mmat(c,\Tmat)$ is linear in $c$ and $\Tmat$ and the set $\mathcal{T}_n$ is convex.
Although, these works on devising a global bound on the Lipschitz constant of a neural network are theoretically interesting, they lack scalability
They can only be computed on small networks and cannot be used during the training of large neural networks for regularization purposes.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Lipschitz Constant of Individual Layers}
\label{subsection:ch3-lipschitz_constant_of_individual_layers}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\noindent
% In order to constrain the Lipschitz constant of neural networks, 
Instead of regularizing the ERM using the global Lipschitz constant, researchers have devised techniques to reduce the Lipschitz constant of \emph{individual layers} instead. 
The global Lipschitz of a neural network can easily be upper bounded by the product of the spectral norm of each weight matrix as follows:
\begin{proposition}[\citet{scaman2018lipschitz}] \label{proposition:ch3-naive_upper_bound_lipschitz}
  Let $N$ be a neural network of $\depth$ layers with 1-Lipschitz activation functions (\eg ReLU,
  Leaky ReLU, Tanh, Sigmoid, etc.), then, the Lipschitz constant of the neural network can be upper bounded as follows:
  \begin{equation} \label{equation:ch3-naive_upper_bound_lipschitz}
    \lip{N} \leq \prod_{i=1}^\depth \norm{\Wmat^{(i)}}_2 \enspace,
  \end{equation}
  where $\Wmat^{(i)}$ are the weights matrices of the neural network.
\end{proposition}

\begin{remark}
  The Lipschitz constant of a layer $\layer_{\Wmat, \bvec}^\rho$ (with a 1-Lipschitz activation function) is equal to the spectral norm of the matrix $\Wmat$ (largest singular value).
  Let $\layer_{\Wmat, \bvec}^\rho: \Rbb^n \rightarrow \Rbb^m$ such that $\layer_{\Wmat, \bvec}^\rho = \rho(\Wmat \xvec + \bvec)$ then by definition of the Lipschitz constant (see \Cref{definition:ch2-lipschitz_constant}) and of the operator norm, we have:
  \begin{equation}
    \lip{\layer_{\Wmat, \bvec}^\rho} = \sup_{\substack{\xvec \in \Rbb^n \\ \xvec \neq 0}} \frac{\norm{\Wmat \xvec}_2}{\norm{\xvec}_2} = \norm{\Wmat}_2
  \end{equation}
  \removespace
\end{remark}

The trivial bound given by the product of layer-wise Lipschitz constants in \Cref{equation:ch3-naive_upper_bound_lipschitz} is known to be loose and pessimistic.
Furthermore, we can show that reducing the Lipschitz constant of each layer independently does not imply that the global Lipschitz constant of the network will be reduced. 




\begin{proposition} \label{proposition:ch3-limit_bound_lipschitz}
  Let $N$ be a neural network, then decreasing the Lipschitz constant of one or more layers does not imply reducing the Lipschitz constant of the network, \ie, $\lip{N}$.
\end{proposition}

% \begin{proposition} \label{proposition:ch3-limit_bound_lipschitz}
%   Let $N: \Rbb^n \rightarrow \Rbb^n$ be a neural network such that $N(\xvec) = \Wmat^{(2)} \rho(\Wmat^{(1)} \xvec)$ with $\Wmat^{(1)}, \Wmat^{(2)} \in \Rbb^{n \times n}$.
%   Then, decreasing the Lipschitz constant of each layer does not imply that the Lipschitz constant of the network will be lower. 
% \end{proposition}

% \begin{proposition} \label{proposition:ch3-limit_bound_lipschitz}
%   Let $N_1(\xvec) = \Amat^{(2)} \rho(\Amat^{(1)} \xvec)$ and $N_2(\xvec) = \Bmat^{(2)} \rho(\Bmat^{(1)} \xvec)$ where $\rho$ is the $\relu$ activation function, then $\norm{\Amat^{(1)}}_2 \leq \norm{\Bmat^{(1)}}_2$ and $\norm{\Amat^{(2)}}_2 \leq \norm{\Bmat^{(1)}}_2$, does not imply that $\lip{N_1} \leq \lip{N_2}$.
% \end{proposition}


\begin{proof}[\Cref{proposition:ch3-limit_bound_lipschitz}]
  Let us prove this claim with a counter-example.
  Let $N_1(\xvec) = \Amat^{(2)} \rho(\Amat^{(1)} \xvec)$ and $N_2(\xvec) = \Bmat^{(2)} \rho(\Bmat^{(1)} \xvec)$ where $\rho$ is the $\relu$ activation function.
  Let
  \begin{align*}
    \Amat^{(1)} &= \leftmatrix 
      \phantom{+}0 & -1 \\ -1 & \phantom{+}0
    \rightmatrix \quad
    \Amat^{(2)}  = \leftmatrix
      -1 & -1 \\ -1 & \phantom{+}0
    \rightmatrix \\
    \Bmat^{(1)} &= \leftmatrix
      \phantom{+}0 & \phantom{+}0 \\ \phantom{+}0 & -1
    \rightmatrix \quad
    \Bmat^{(2)} = \leftmatrix
      -1 & -1 \\ -1 & -1
    \rightmatrix
  \end{align*}
  then: \vspace{-0.5cm}
  \begin{equation*}
    \norm{\Amat^{(1)}}_2 = 1,\ \norm{\Amat^{(2)}}_2 = \sqrt{2}
    \quad \text{and} \quad
    \norm{\Bmat^{(1)}}_2 = 1,\ \norm{\Bmat^{(2)}}_2 = 2
  \end{equation*}
  From \Cref{theorem:ch3-lipschitz_differential_op} and the chain rule, the Lipschitz constant of the networks $N_1$ and $N_2$ can be expressed as follows:
  \begin{align*}
    \lip{N_1} &= \sup_{\xvec \in [0, 1]^2} \norm{\Amat^{(2)} \diag\left(\xvec\right) \Amat^{(1)}}_2 \\
    \lip{N_2} &= \sup_{\xvec \in [0, 1]^2} \norm{\Bmat^{(2)} \diag\left(\xvec\right) \Bmat^{(1)}}_2
  \end{align*}
  It is easy to verify that:
  \begin{equation*}
    \lip{N_1} = \frac{1 + \sqrt{5}}{2} \approx 1.618 \quad \text{and} \quad \lip{N_2} = \sqrt{2} \approx 1.414
  \end{equation*}
  which concludes the proof.
\end{proof}
\noindent
While we cannot have a guarantee that the global Lipschitz will be reduced, we could still have an idea of the value of the global Lipschitz with the upper bound presented in~\Cref{equation:ch3-naive_upper_bound_lipschitz}.


\citet{huster2018limitations} have demonstrated several limitations on the expressive power of neural networks where the product of layer-wise Lipschitz constants is constrained.
In the same vein, \citet{couellan2019coupling} empirically showed that Lipschitz Regularization offers a trade-off between adversarial robustness and expressivity of the network.
However, the bound in \Cref{equation:ch3-naive_upper_bound_lipschitz} appears in multiple generalization bound~\cite{neyshabur2017,bartlett2017spectrally,golowich2018} and adversarial generalization~\cite{farnia2018generalizable} (see \Cref{chapter:ch2-background}) which could suggest that reducing the bound would improve the generalization capabilities of neural networks and its robustness.

Based on this theoretical insight, researchers have developed several techniques to constrain the Lipschitz constant of each layer in order to improve the generalization and robustness of neural networks.
A technique to enforce 1-Lipschitz layers is to impose or promote an orthogonality constrain of the weight matrices.
A square orthogonal matrix $\Mmat$ is a matrix whose columns and rows are orthogonal unit vectors and all eigenvalues are equal to 1.
\citet{cisse2017parseval} and more recently \citet{wang2020orthogonal,huang2020controllable} have proposed to minimize the following term:
\begin{equation} \label{equation:ch3-orthogonality_constraint}
  \frac{\beta}{2} \norm{\Wmat^\top \Wmat - \Imat}_2  \enspace, 
\end{equation}
to promote the orthogonality constraint, in addition to the usual loss function:
In the above equation, the hyper-parameter $\beta$ controls the constraint.
A higher $\beta$ would lead to a better orthogonality constraint and therefore, a Lipschitz constant ``almost'' equal to 1 for all the layers.

On the other hand, \citet{anil2019sorting} proposed to enforce the orthogonality of weight matrices by directly optimizing on the Stiefel
manifold (\ie, the manifold of orthogonal matrices, see~\citet{absil2009optimization}).
To perform this optimization, they made use of an iterative algorithm first introduced by~\citet{bjorck1971iterative}.
For a given matrix $\Wmat = \Wmat^{(0)}$, the algorithm finds the closest orthonormal matrix by computing the following term:
\begin{equation}
  \Wmat^{(k+1)} = \Wmat^{(k)} \left( \Imat + \frac{1}{2} \Vmat^{(k)} + \cdots + (-1)^r {-\frac{1}{2} \choose r}  \left(\Vmat^{(k)}\right)^r \right)
\end{equation}
where $\Vmat = \Imat - \Wmat^{(k)\top} \Wmat^{(k)}$.
Although this algorithm works well on dense matrices, it can be difficult to apply it to convolutions. 
\citet{li2019preventing} built upon this idea and proposed an algorithm to enforce the orthogonality of convolutional layers.
They used the orthogonal projection proposed by \citet{kautsky1994matrix} and \citet{xiao2018dynamical} to build convolutional neural networks with orthogonal convolutions.

All techniques that impose an orthogonality constraint on the weights matrices successfully reduce the Lipschitz constant of the layers of the networks.
Moreover, when the Lipschitz constants of all the layers are low, we could have an idea of the value of the global Lipschitz with the upper bound of~\Cref{equation:ch3-naive_upper_bound_lipschitz} (\ie, if Lipschitz constant of all the layers are equal to 1, then, the network is $1$-Lipschitz).
However, enforcing the orthogonality constraint, either by regularizing with the term of~\Cref{equation:ch3-orthogonality_constraint} or by optimizing on the Stiefel manifold, is the costly operation which make it difficult to scale on large neural networks.




\begin{algorithm}[tb]
  \caption{Power method for producing the largest singular value, $\sigma_1$, of a non-square matrix, $\Wmat$ \cite{gouk2018regularisation,golub2000eigenvalue}}
  \begin{algorithmic}[1]
    \Require{affine function $f(\xvec) = \Wmat \xvec + \bvec$, number of iteration $N$}
    \Ensure{approximation of the Lipschitz constant $\lip{f}$}
    \State Randomly initialise $\xvec$
    \For{$i = 1$ \textbf{to} $N$}
      \State $\xvec \gets \Wmat^\top \Wmat \xvec / \norm{\xvec}_2$
    \EndFor
    \State \textbf{return} $\norm{\Wmat \xvec}_2 / \norm{\xvec}_2$
  \end{algorithmic}
  \label{algorithm:ch3-power_method}
\end{algorithm}

\begin{algorithm}[tb]
  \caption{Convolutional power method \cite{farnia2018generalizable}}
  \begin{algorithmic}[1]
    \Require{2d-convolution function $f: \Rbb^{n \times n} \rightarrow \Rbb^{m \times m}$ with kernel $k$, 2d-convolution-transpose function $g: \Rbb^{n \times n} \rightarrow \Rbb^{m \times m}$ with kernel $k$ number of iteration $N$}
    \Ensure{approximation of the Lipschitz constant $\lip{f}$}
    \State Initialize $\xvec$ with a random vector matching the shape of the convolution input
    \For{$i = 1$ \textbf{to} $N$}
      \State $\xvec \gets f(\xvec) / \norm{f(\xvec)}_2 $
      \State $\xvec \gets g(\xvec) / \norm{g(\xvec)}_2$
    \EndFor
    \State \textbf{return} $\norm{f(\xvec)}_2 / \norm{\xvec}_2$
  \end{algorithmic}
  \label{algorithm:ch3-power_method_generic}
\end{algorithm}


Another technique, called \emph{Spectral Normalization}, consists in normalizing each weight matrix by its largest singular value, thus imposing each layer to be 1-Lipschitz.
As with the orthogonality constraint, this technique leads the network to have a global Lipschitz constant of 1.
\citet{yoshida2017spectral} were the first to propose this method to improve the generalization of neural networks followed by~\cite{miyato2018spectral,gouk2018regularisation,farnia2018generalizable} for improving generalization and robustness against adversarial attacks.
In order to perform spectral normalization, they divided the values of each weight matrix by an approximation of its largest singular value.
The approximation of the largest singular was computed using the power method~\cite{golub2000eigenvalue}.

% used the power method~\cite{golub2000eigenvalue} to compute an approximation of the largest singular value of each weight matrix and divided all the values of the weight matrix by this 

The power method is an iterative eigenvalue algorithm (also known as the Von Mises iteration \cite{mises1929praktische}).
Given a matrix $\Wmat$ and a random vector $\bvec^{(0)}$, the eigenvector associated with the largest eigenvalue of the matrix $\Wmat$ can be computed with the following recurrence relation:
\begin{equation}
  \bvec^{(k+1)} = \frac{\Wmat \bvec^{(k)}}{\norm{\bvec^{(k)}}_2}  
\end{equation}
Then, the largest eigenvalue (when we talk about ``largest eigenvalue'' we mean in absolute value) can be optained with the \emph{Rayleigh quotient}:
\begin{equation}
  \sigma_1\left( \Wmat \right) = \frac{\bvec^{(k)\top} \Wmat \bvec^{(k)}}{\bvec^{(k)\top} \bvec^{(k)}}
\end{equation}
With a sufficient number of iterations, the algorithm provably converges to the largest eigenvalue of the matrix.
To find the largest singular value, we can leverage the relation between eigenvalues and singular values:
\begin{equation}
  \sigma \left( \Wmat \right) = \sqrt{ \lambda \left( \Wmat^\top \Wmat \right) }
\end{equation}
The rate of convergence of the algorithm depends on the ratio between the second-largest eigenvalue and the largest eigenvalue.
Indeed, a high ratio can lead to slow convergence.
The pseudocode of the power method is given in \Cref{algorithm:ch3-power_method}.
Altough, \Cref{algorithm:ch3-power_method} needs explicit matrix for computing the largest singular value, \citet{farnia2018generalizable,ryu2019plug} extended the power method to convolutional layers where the matrix $\Wmat$ is not explicitly constructed.
The pseudocode of their method is presented in \Cref{algorithm:ch3-power_method_generic}. 

In the context of deep learning and spectral normalization, the largest singular value needs to be computed for each layer of the network at each step of the training. 
Given that current state-of-the-art architecture have between 50 and 100 layers \cite{he2016deep,tan2019efficientnet}, using the power method \emph{until convergence} is prohibitive.
In~\Cref{chapter:ch5-lipschitz_bound}, we propose a new regularization scheme for reducing the Lipschitz constant of individual layers.
We will shown in~\Cref{subsection:ch5-comparison_of_lipbound_with_other_state-of-the-art_approaches} that our approach is more efficient that the power method even with a small number of iterations.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Singular Values of Convolutional Layers}
\label{subsection:ch3-singular_values_of_convolutional_layers}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The power method is not the only technique available for approximating the largest singular value (Lipschitz constant) of a convolutional layer.
Several works have devised bounds or approximations on the largest singular value of convolutional layers by exploiting the \emph{structure} of the convolution operation \cite{sedghi2018singular,bibi2019deep,singla2019bounding,jia2017improving}.

% \citet{sedghi2018singular} have observed that a doubly-block Toeplitz matrix can be approximated by a 
%
% \citet{sedghi2018singular} have exploited the properties 

% \citet{gray2006toeplitz} have observed that band-Toeplitz matrices can be `approximated' by band-circulant matrices.
% This `approximation' is formalized by a mathematical concept called \emph{}j

To approximate the singular values of a convolutional layer, \citet{sedghi2018singular} have exploited the properties of doubly-block circulant matrices (\ie, a circulant block matrix where each block is also a circulant matrix).
Indeed, a doubly-block circulant matrix is the matrix representation of a convolution with circulant padding.
In their work, \citet{sedghi2018singular} assume that the properties of doubly-block circulant matrices are `close' to the properties of a doubly-block Toeplitz matrix.

To compute the singular values of doubly-block circulant matrices, \citet{sedghi2018singular} have demonstrated the following result:
\begin{theorem}[Theorem 5 from \citet{sedghi2018singular}] \label{theorem:ch3-singular_values_doubly_block_circulant}
  Let $\Amat$ be a doubly-block circulant matrix such that:
  \begin{equation*}
    \Amat = \leftmatrix
      \Cmatsf^{(0)}   & \Cmatsf^{(n-1)} & \Cmatsf^{(n-2)} & \cdots        & \cdots          & \Cmatsf^{(1)}   \\
      \Cmatsf^{(1)}   & \Cmatsf^{(0)}   & \Cmatsf^{(n-1)} & \ddots        &                 & \vdots          \\
      \Cmatsf^{(2)}   & \Cmatsf^{(1)}   & \ddots          & \ddots        & \ddots          & \vdots          \\
      \vdots          & \ddots          & \ddots          & \ddots        & \Cmatsf^{(n-1)} & \Cmatsf^{(n-2)} \\
      \vdots          &                 & \ddots          & \Cmatsf^{(1)} & \Cmatsf^{(0)}   & \Cmatsf^{(n-1)} \\
      \Cmatsf^{(n-1)} & \cdots          & \cdots          & \Cmatsf^{(2)} & \Cmatsf^{(1)}   & \Cmatsf^{(0)}
    \rightmatrix
  \end{equation*}
  where $\Cmatsf^{(i)} = \circulant({\cvec_i}),\ \forall i \in \Iset_n^+$.
  Let $\Kmat = \leftmat \cvec_0, \cvec_1, \cdots, \cvec_{n-1} \rightmat^\top$ then, the singular values of the doubly-block circulant matrix $\Amat$ are the modulus of the entries of $\Umat_n^\top \Kmat \Umat_n$.
\end{theorem}


\noindent
To prove \Cref{theorem:ch3-singular_values_doubly_block_circulant}, \citet{sedghi2018singular} used the diagonalization of doubly-block circulant matrices (see~\Cref{chapter:ch2-background}, \Cref{equation:ch2-diagonalization_doubly_block_circulant_matrix}).
The main advantage of this approach is that the singular values of a doubly-block circulant matrix can be computed with the Fast Fourier Transform algorithm (see \Cref{section:ch2-a_primer_on_circulant_and_toeplitz_matrices}) which offers a reduced complexity compared to classical approaches for computing the singular values of a matrix.
However, this approach exhibits several limitations.
First, this method results in a loose approximation of the maximal singular value of a convolutional layer which does not use the circulant padding which is often the case in practical settings.
Also, the complexity of their algorithm is dependent on the size of the input which can be high for large datasets.
Finally, for multi-channel convolution, their method requires the computation of the spectral norm of $n^2$ matrices each of size $\cin \times \cout$ as stated in the following theorem:

\begin{theorem}[Theorem 6 from \citet{sedghi2018singular}]
  Let $\Mmat$ be the matrix encoding the linear transform computed by a multi-channel convolutional layer.
  Let $\Kmat \in \Rbb^{\cin \times \cout \times n \times n}$ such that $(\Kmat)_{i,j}$ for all $i,j$ be constructed as in \Cref{theorem:ch3-singular_values_doubly_block_circulant}, 
  Let $\widetilde{\Kmat}_{i,j} = \Umat^\top \leftmat \Kmat \rightmat_{i,j} \Umat_n $ and define the following operator matrix 
  \begin{equation}
    \Pmat(i,j) = \leftmatrix 
    \leftmat (\widetilde{\Kmat})_{(0,0)} \rightmat_{i, j} & \cdots & \leftmat (\widetilde{\Kmat})_{(0, \cout-1)} \rightmat_{i, j} \\
    \vdots & & \vdots \\
    \leftmat (\widetilde{\Kmat})_{(\cin-1, 0)} \rightmat_{i, j} & \cdots & \leftmat (\widetilde{\Kmat})_{(\cin-1, \cout-1)} \rightmat_{i, j}
    \rightmatrix
  \end{equation}
  Then
  \begin{equation}
    \sigma(\Mmat) = \bigcup_{i, j = 0}^{n-1} \sigma \left(  \Pmat(i,j) \right).
  \end{equation}
  \removespace
\end{theorem}



% , their method requires the computation of the spectral norm of n
% 2 matrices (each matrix of
% size cout × cin) for every convolution layer making it impractical to use during training.


In the same vein, \citet{singla2019bounding} have used the properties of convolutions to devise several bounds on the singular values of convolution layers.
Recall from \Cref{subsubsection:ch2-relation_with_the_convolution_operator} that a convolution kernel is a 4 dimensional tensor of size $\cout \times \cin \times k_1 \times k_2$.
\citet{singla2019bounding} have demonstrated that the largest singular value of a convolution layer $\layer_\Kmat$ parameterized by a kernel $\Kmat$ can be upper-bounded as follows:

\begin{theorem}[Reformulation of Theorem 1 from \citet{singla2019bounding}]
  Let $\Kmat \in \Rbb^{\cout \times \cin \times k_1 \times k_2}$ be the kernel of a convolution layer $\layer_\Kmat$, then,
  \begin{equation}
    \lip{\layer_\Kmat} \leq \min \left\{ \sqrt{k_1 k_2} \norm{\Rmat}_2, \sqrt{k_2 k_2} \norm{\Smat}_2 \right\}
  \end{equation}
  where $\Rmat$ and $\Smat$ are matrices of size $k_1 \cout \times k_2 \cin$ and $k_2 \cout \times k_1 \cin$ defined as follows:
  \begin{align}
    \Rmat &= \leftmatrix
      % \Kmat_{0,0,:,:}       & \Kmat_{0,1,:,:}         & \cdots & \Kmat_{0,c_{in}-1,:,:} \\
      % \Kmat_{1,0,:,:}       & \Kmat_{1,1,:,:}         & \cdots & \Kmat_{1,c_{in}-1,:,:} \\
      % \vdots                & \vdots                  & \ddots & \vdots                 \\
      % \Kmat_{\cout-1,0,:,:} & \Kmat_{\cout-1,1,:,:} & \cdots & \Kmat_{\cout-1,\cin-1,:,:}
      (\Kmat)_{0,0}       & \cdots & (\Kmat)_{0,\cin-1} \\
      (\Kmat)_{1,0}       & \cdots & (\Kmat)_{1,\cin-1} \\
      \vdots              & \ddots & \vdots             \\
      (\Kmat)_{\cout-1,0} & \cdots & (\Kmat)_{\cout-1,\cin-1}
    \rightmatrix \\[0.5cm]
    \Smat &= \leftmatrix
      (\Kmat)_{0,0}^\top       & \cdots & (\Kmat)_{0,\cin-1}^\top \\
      (\Kmat)_{1,0}^\top       & \cdots & (\Kmat)_{1,\cin-1}^\top \\
      \vdots                   & \ddots & \vdots                  \\
      (\Kmat)_{\cout-1,0}^\top & \cdots & (\Kmat)_{\cin-1,\cout-1}^\top
    \rightmatrix
  \end{align}
  \removespace
\end{theorem}

In order to prove this result, \citet{singla2019bounding} built upon the work of \citet{sedghi2018singular} and have also only considered circulant convolutions (performed by doubly-block circulant matrices).
Instead of proposing a method to compute \emph{all} singular values of the equivalent doubly-block circulant matrix, their method is an upper-bound on the largest singular value of the Jacobian of the convolution. 
Because this method is independent of the input dimension, the computational complexity is substantially reduced compared to the approach of \citet{sedghi2018singular}, however, the reduction in computational complexity is at the expense of accuracy as we will show in~\Cref{chapter:ch5-lipschitz_bound}.
 

% => link between margin and robustness
% - \citet{tsuzuku2018lipschitz}: use power method but do not normalize

% =>> GLOBAL BOUND
%
% => global bound / semi-definite programming
% - \citet{scaman2018lipschitz}
% - \cite{fazlyab2019efficient}
% - \citet{latorre2020lipschitz}
%
% =>> POWER METHOD 
% => spectral normalization (they all use the power method \citet{golub2000eigenvalue})
% - first paper on spectral normalization \citet{gouk2018regularisation}
% - \citet{yoshida2017spectral,miyato2018spectral}: with power method
% - \citet{farnia2018generalizable}: with power method specific for convolutional layers
% - \citet{tsuzuku2018lipschitz}: use power method but do not normalize
%
% =>> WORK ON CONVOLUTION
%
% => orthogonal convolutions
% - \citet{cisse2017parseval}
% - \citet{li2019preventing}
% - \citet{wang2020Orthogonal} (orthogonal convolution)
%
% => Upper bound on convolution
% - \citet{sedghi2018singular}
% - \citet{singla2019bounding}


% The product of the Lipschitz constant of each layer is an upper-bound for the Lipschitz constant of the entire network, and it can be used as a surrogate to perform Lipschitz regularization.
% Since most common activation functions (such as ReLU) have a Lipschitz constant equal to one, the main bottleneck is to compute the Lipschitz constant of the underlying linear application which is equal to its maximal singular value.
% The work in this line of research mainly relies on the celebrated iterative algorithm by~\citet{golub2000eigenvalue} used to approximate the maximum singular value of a linear function.

% The last few years have witnessed a growing interest in Lipschitz regularization of neural networks, with the aim of improving their generalization~\cite{bartlett2017spectrally}, their robustness to adversarial attacks~\cite{tsuzuku2018lipschitz, farnia2018generalizable}, or their generation abilities (\eg for GANs: \citealt{miyato2018spectral,arjovsky2017wasserstein}).
% Unfortunately computing  the exact Lipschitz constant of a neural network is NP-hard~\cite{scaman2018lipschitz} and in practice, existing techniques such as~\citet{scaman2018lipschitz, NIPS2019_9319} or~\citet{latorre2020lipschitz} are difficult to implement for neural networks with more than one or two layers, which hinders their use in deep learning applications.

% To overcome this difficulty, most of the work has focused on computing the Lipschitz constant of \emph{individual layers} instead.
% The product of the Lipschitz constant of each layer is an upper-bound for the Lipschitz constant of the entire network, and it can be used as a surrogate to perform Lipschitz regularization.
% Since most common activation functions (such as ReLU) have a Lipschitz constant equal to one, the main bottleneck is to compute the Lipschitz constant of the underlying linear application which is equal to its maximal singular value.
% The work in this line of research mainly relies on the celebrated iterative algorithm by~\citet{golub2000eigenvalue} used to approximate the maximum singular value of a linear function.
% Although generic and accurate, this technique is also computationally expensive, which impedes its usage in large training settings. 











