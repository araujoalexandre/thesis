%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Background}
\label{chapter:background}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\localtableofcontents



% \citeauthor{werbos1974thesis,rumelhart1986learning} conside
%
% While connectionism approaches were abandoned for the next few years, Paul Werbos revived the field with the discovery of the \emph{backpropagation} algorithm \cite{werbos1974thesis} which as been popularized later on by \citet{rumelhart1986learning}.
%
% During the training of the neural network, the backpropagation algorithm \emph{efficiently} computes the gradient of the loss function we seek to minimize allowing the training of multi layer neural network. In practice, the backpropagation algorithm is simply an application of the chain rule for the composition of function. If $h$, $f$ and $g$ are differentiable functions and $h(x) = (f \circ g)(x)$ then:
% \begin{equation}
%   h'(x) = (f \circ g)' = (f' \circ g) \cdot g'.
% \end{equation}

% This discovery led to the first large scale application with the US Post Office. Yann Lecun successfully devise a Convolutional Neural Network to recognize hand written digits \cite{lecun1998gradient}.




% \section{From Neural Networks to Deep Learning}
%
% Neural Networks find they roots, in 1958, in the works of Frank Rosenblatt \cite{rosenblatt1958perceptron} where for the first time, the Perceptron, an electronic device inspired by the human brain, showed ability to \emph{learn} from multiples examples.
% This work had greatly advanced the concepts of connectionism \cite{medler1998brief} and was described as revolutionary by the New York Times \todo{cite} \cite{}. 
%
%
% \todo{detail limit sign(f(x))}
%
% While connectionism approaches were abandoned for the next few years, Paul Werbos revived the field with the discovery of the \emph{backpropagation} algorithm \cite{werbos1974thesis} which as been popularized later on by \citet{rumelhart1986learning}.
%
% During the training of the neural network, the backpropagation algorithm \emph{efficiently} computes the gradient of the loss function we seek to minimize allowing the training of multi layer neural network. In practice, the backpropagation algorithm is simply an application of the chain rule for the composition of function. If $h$, $f$ and $g$ are differentiable functions and $h(x) = (f \circ g)(x)$ then:
% \begin{equation}
%   h'(x) = (f \circ g)' = (f' \circ g) \cdot g'.
% \end{equation}
%
% This discovery led to the first large scale application with the US Post Office. Yann Lecun successfully devise a Convolutional Neural Network to recognize hand written digits \cite{lecun1998gradient}.
%
%
% \section{The Limitations of Deep Neural Networks}
% xxx
%
%
% AlexNet \cite{NIPS2012_4824}
% VGG \cite{}

